{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "## 1.  If there is a conflict between the problem description in the ipython notebook and the question in the pdf, follow the later one.\n",
    "## 2. Please search 'Code Clip' to find the part you need to fill in. After you finish the required part, you may need to run other related code to evaluate or visualization.\n",
    "## 3. If you have a better implementation or find mistakes in this notebook, you could add/modify any function (input, output, and return) yourself. Everything is flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modification v1\n",
    "This modification will not influence those who already finished the homework. For those who have not, we do two changes:\n",
    "1. Delete all the content in function `run_cross_validation_on_trees` and `compare`. \n",
    "2. Change the `sm_tree_depths = range(1,10)` to `sm_tree_depths = range(1,11)` to keep the same as the pdf version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import accuracy_score, auc,roc_curve\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from scipy.stats import ttest_ind\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training and Testing Data. Get a initial statistics of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data_train.csv')\n",
    "test_data = pd.read_csv('./data_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features_mean = list(train_data.columns[1:31])\n",
    "\n",
    "\n",
    "X_train = train_data.loc[:,features_mean]\n",
    "y_train = train_data.loc[:, 'diagnosis']\n",
    "\n",
    "X_test = test_data.loc[:,features_mean]\n",
    "y_test = test_data.loc[:, 'diagnosis']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Balanced Dataset\n",
    "\n",
    "###  3.1.1 Use 5-fold cross validation on the training set only, and compare accuracy and time cost performance of three different algorithms:  ID3, CART and Random Forest, \n",
    "\n",
    "\n",
    "Code Clip 3.1.1a: Complete the function `compare`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def accuracy_per_class(predict, label):\n",
    "    cm = confusion_matrix(label, predict)\n",
    "    return np.diag(cm)/np.sum(cm, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "start = time.time()\n",
    "\n",
    "def compare(X_train, y_train, X_test, y_test, class_weight = None, max_depth = None):\n",
    "    \n",
    "    accuracy_all = []\n",
    "    accuracy_per_class_all = []\n",
    "    cvs_all = []\n",
    "    \n",
    "    X = np.concatenate([X_train, X_test], axis= 0)\n",
    "    y = np.concatenate([y_train, y_test], axis= 0)\n",
    "\n",
    "    # Code Clip 3.1.1a\n",
    "    #-------------Forest-------------\n",
    "    # start = time.time()\n",
    "    # Step 1: Build a random forest and fit the input data.\n",
    "    # Step 2: Do prediction on the testing set.\n",
    "    # end = time.time()\n",
    "    # Step 3: Calculte different metrics: accuracy over testing set, \n",
    "    #             corss validation score over the whole training set,\n",
    "    #             accuracy of each class.\n",
    "    model1 = RandomForestClassifier(n_estimators=50,max_depth = None,min_samples_split=2,random_state = 0)\n",
    "    scores = cross_val_score(model1,X_train,y_train,cv=5)\n",
    "    y_preds=model1.fit(X_train,y_train).predict(X_test)\n",
    "\n",
    "    accuracy_all.append(accuracy_score(y_test,y_preds))\n",
    "    scores=np.array(scores).mean()\n",
    "    cvs_all.append(scores)\n",
    "    accuracy_per_class(y_preds,y_test)\n",
    "    accuracy_per_class_all.append(accuracy_per_class)\n",
    "    print(cvs_all)\n",
    "    print(accuracy_per_class)\n",
    "    print(accuracy_all)\n",
    "    end = time.time()\n",
    "    running_time = end-start\n",
    "    print('RF time cost : %.5f sec' %running_time)\n",
    "    #-------------Split by GINI-------------\n",
    "    model2 = DecisionTreeClassifier()\n",
    "    scores2 = cross_val_score(model2,X_train,y_train,cv=5)\n",
    "    y_preds2=model2.fit(X_train,y_train).predict(X_test)\n",
    "\n",
    "    accuracy_all.append(accuracy_score(y_test,y_preds2))\n",
    "    scores2=np.array(scores2).mean()\n",
    "    cvs_all.append(scores2)\n",
    "    accuracy_per_class(y_preds2,y_test)\n",
    "    accuracy_per_class_all.append(accuracy_per_class)\n",
    "    print(cvs_all)\n",
    "    print(accuracy_per_class)\n",
    "    print(accuracy_all)\n",
    "    end = time.time()\n",
    "    running_time = end-start\n",
    "    print('GINI time cost : %.5f sec' %running_time)\n",
    "\n",
    "    #-------------Split by Entropy-------------\n",
    "    model3 = DecisionTreeClassifier(criterion='entropy')\n",
    "    scores3 = cross_val_score(model3,X_train,y_train,cv=5)\n",
    "    y_preds3=model3.fit(X_train,y_train).predict(X_test)\n",
    "\n",
    "    accuracy_all.append(accuracy_score(y_test,y_preds3))\n",
    "    sd=np.std(scores3)\n",
    "    scores3=np.array(scores3).mean()\n",
    "    #sd=np.std(scores3)\n",
    "    print(sd)\n",
    "    cvs_all.append(scores3)\n",
    "    accuracy_per_class(y_preds3,y_test)\n",
    "    accuracy_per_class_all.append(accuracy_per_class)\n",
    "    print(cvs_all)\n",
    "    print(accuracy_per_class)\n",
    "    print(accuracy_all)\n",
    "    end = time.time()\n",
    "    running_time = end-start\n",
    "    print('Entropy time cost : %.5f sec' %running_time)\n",
    "    \n",
    "#     return accuracy_all, accuracy_per_class_all, cvs_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9537468811381855]\n",
      "<function accuracy_per_class at 0x126dc2950>\n",
      "[0.9649122807017544]\n",
      "RF time cost : 0.83522 sec\n",
      "[0.9537468811381855, 0.9121314434357911]\n",
      "<function accuracy_per_class at 0x126dc2950>\n",
      "[0.9649122807017544, 0.9385964912280702]\n",
      "GINI time cost : 0.87361 sec\n",
      "0.01798855694897011\n",
      "[0.9537468811381855, 0.9121314434357911, 0.9164320220841959]\n",
      "<function accuracy_per_class at 0x126dc2950>\n",
      "[0.9649122807017544, 0.9385964912280702, 0.956140350877193]\n",
      "Entropy time cost : 0.91225 sec\n"
     ]
    }
   ],
   "source": [
    "compare(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Clip 3.1.1b: Do pairwise t-tests to determine whether there’s a significant difference between the best algorithm and the other algorithms. (Hint: You could first use `sklearn.model_selection.KFold` to get 5 different train/val division on the training set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Clip 3.1.1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.1.2 Effect of the depth.\n",
    "\n",
    "Code Clip 3.1.2: Complete the function `run_cross_validation_on_trees`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cross_validation_on_trees(X, y, tree_depths, cv=5, scoring='accuracy'):\n",
    "#     cv_scores_list = []\n",
    "#     cv_scores_std = []\n",
    "#     cv_scores_mean = []\n",
    "#     accuracy_scores = []\n",
    "    \n",
    "#     for depth in tree_depths:\n",
    "        # Get the accuracy,  mean, std of the cross validation score.\n",
    "        \n",
    "\n",
    "    # return what you need\n",
    "#     return cv_scores_mean, cv_scores_std, accuracy_scores\n",
    "  \n",
    "# function for plotting cross-validation results\n",
    "def plot_cross_validation_on_trees(depths, cv_scores_mean, cv_scores_std, accuracy_scores, title):\n",
    "    fig, ax = plt.subplots(1,1, figsize=(15,5))\n",
    "    ax.plot(depths, cv_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)\n",
    "    ax.fill_between(depths, cv_scores_mean-2*cv_scores_std, cv_scores_mean+2*cv_scores_std, alpha=0.2)\n",
    "    ylim = plt.ylim()\n",
    "    ax.plot(depths, accuracy_scores, '-*', label='train accuracy', alpha=0.9)\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.set_xlabel('Tree depth', fontsize=14)\n",
    "    ax.set_ylabel('Accuracy', fontsize=14)\n",
    "    ax.set_ylim([0.8,1])\n",
    "    ax.set_xticks(depths)\n",
    "    ax.legend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_tree_depths = range(1,11)\n",
    "# get what you returned.\n",
    "run_cross_validation_on_trees(X_train, y_train, sm_tree_depths)\n",
    "\n",
    "\n",
    "# plotting accuracy\n",
    "plot_cross_validation_on_trees(sm_tree_depths, sm_cv_scores_mean, sm_cv_scores_std, sm_accuracy_scores, \n",
    "                               'Accuracy per decision tree depth on training data')\n",
    "plt.show()\n",
    "\n",
    "# Play: Only uses the first ten features.\n",
    "# sm_cv_scores_mean2, sm_cv_scores_std2, sm_accuracy_scores2 = run_cross_validation_on_trees(X_train.iloc[:,:10],\n",
    "#                                                                                         y_train, sm_tree_depths)\n",
    "\n",
    "# # plotting accuracy\n",
    "# plot_cross_validation_on_trees(sm_tree_depths, sm_cv_scores_mean2, sm_cv_scores_std2, sm_accuracy_scores2, \n",
    "#                                'Accuracy per decision tree depth on training data')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 ROC and vairable importance\n",
    "\n",
    "Code Clip 3.1.3a: Complete the function `draw_roc_with_feature_idx`. Then finished the next two steps (`AUC of different features` and `Draw ROC Curve of the first five features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(fpr, tpr, roc_auc, title = ''):\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area ={})'.format(roc_auc))\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic over {}'.format(title))\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "def draw_roc_with_feature_idx(X_test, y_test, i, draw = False):\n",
    "    \n",
    "    # Code Clip 3.1.3\n",
    "    # calculate list of false positive rate and true positive rate.\n",
    "    # calculate AUC.\n",
    "    \n",
    "    \n",
    "    if draw:\n",
    "        plot_roc(fpr, tpr, roc_auc, title = 'Feature' + str(i))\n",
    "        \n",
    "    return auc\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC of different features\n",
    "\n",
    "Code clip 3.1.3b,\n",
    "You may find plt.bar useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "# Code Clip 3.1.3b\n",
    "\n",
    "plt.xlabel('Feature Number')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('AUC of different features')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Draw ROC Curve of the first five features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    draw_roc_with_feature_idx(X_test, y_test, i, draw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 Partial ROC\n",
    "Code Clip 3.1.4 Follow the intruction in the quesion. You could test the correctness by setting $t_0 = 0, t_1= 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Clip 3.1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Imbalanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data_imbalanced_train.csv')\n",
    "test_data = pd.read_csv('./data_imbalanced_test.csv')\n",
    "\n",
    "features_mean = list(train_data.columns[1:31])\n",
    "\n",
    "X_train = train_data.loc[:,features_mean]\n",
    "y_train = train_data.loc[:, 'diagnosis']\n",
    "\n",
    "X_test = test_data.loc[:,features_mean]\n",
    "y_test = test_data.loc[:, 'diagnosis']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1： What is the ratio between the two labels ?\n",
    "Code Clip 3.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Clip 3.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Use three algorithms  from  Problem  3.1  to  train  models  on  the  training  set.   Please  report  the confusion matrix on the test set.\n",
    "Code Clip 3.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Clip 3.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 For each class, get the accuracy on the training set and testing set with different sample reweighting parameters. Plot them according to the reweight parameter. (Set the maximum depth of all the algorithms to 3). \n",
    "Code Clip 3.2.3:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_list = list(np.arange(1, 15))\n",
    "\n",
    "\n",
    "for weight in weight_list:\n",
    "    # Code Clip 3.2.3\n",
    "    # Calculte the accuracy of each class.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
